#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug  2 20:54:14 2024

@author: charmainechia
"""

import numpy as np
import pandas as pd
import matplotlib as mpl
from scipy.stats import rankdata
import matplotlib.pyplot as plt
from variables import yvar_list_key, xvar_list_dict
from model_utils import plot_feature_importance_heatmap
from plot_utils import figure_folder, heatmap
from get_datasets import data_folder, get_XYdata_for_featureset
from utils import get_high_correlation_pairs

def parse_model_type(f):
    if f.find('randomforest')>-1:
        model_type = 'randomforest'
    elif f.find('lasso')>-1:
        model_type = 'lasso'
    elif f.find('plsr')>-1:
        model_type = 'plsr'
    return model_type
                
def adapt_dfrow(dfrow_to_adapt, input_suffix, output_suffix, xvar_list_final, xvar_to_ignore=['DO', 'feed %', 'feed vol', 'pH']): 
    dfrow_final = np.zeros((len(xvar_list_final),))
    dfrow_final[:] = np.nan
    for i, xvar_base in enumerate(xvar_list_final): 
        for s in output_suffix:
            xvar_base = xvar_base.replace(s, input_suffix)
        if xvar_base in dfrow_to_adapt and xvar_base not in xvar_to_ignore:
            val = np.abs(dfrow_to_adapt[xvar_base])
            dfrow_final[i] = val
    return dfrow_final

def get_ranking_arr_across_row(val_arr):
    # get rankings by row
    ranking_arr = np.zeros_like(val_arr)
    ranking_arr[:] = np.nan
    for row_idx in range(val_arr.shape[0]):
        ranking_arr[row_idx,:] = val_arr.shape[1]+1-rankdata(val_arr[row_idx,:])
    # set nan elements to NaN
    ranking_arr[np.isnan(val_arr)] = np.nan
    return ranking_arr

def get_contents_of_brackets(string): 
    content_list = []
    string_to_search = string
    no_more_brackets = False
    while len(string_to_search)>3 and not no_more_brackets:
        startidx = string_to_search.find('(')
        if startidx > -1: 
            endidx = string_to_search[startidx:].find(')')
            content = string_to_search[startidx+1:startidx+endidx]
            content_list.append(content)
            string_to_search = string_to_search[startidx+endidx:]
        else: 
            no_more_brackets = True
    return content_list

def plot_colorcoded_barplot(arr, xvar_list, width=0.8, figsize=(10,5), color_list='b', annotate_xvar=None, figtitle=None, savefig=None):
    # plot barplot
    fig, ax = plt.subplots(1,1, figsize=figsize)
    xtickpos = np.arange(len(xvar_list))
    ax.bar(xtickpos, arr, width=width, color=color_list)
    ax.set_xticks(xtickpos, xvar_list, fontsize=7, rotation=90)
    if figtitle is None:
        ax.set_title(yvar, fontsize=20)
    else: 
        ax.set_title(figtitle, fontsize=20)
    ax.set_ylabel('feature importances', fontsize=16)
    # annotate xvar with lasso features, if needed
    if annotate_xvar is not None: 
        (ymin, ymax) = ax.get_ylim()
        ax.scatter(annotate_xvar, arr[annotate_xvar]+(ymax-ymin)/25, color='k', s=2)
    if savefig is not None:
        plt.savefig(savefig, bbox_inches='tight', dpi=300)
    plt.show()
    
def get_color_list_bycluster(cluster_df, xvar_list, num_clusters=24, ncolors_in_palette=10): 
    cluster_labels = cluster_df.loc[num_clusters,xvar_list].to_numpy()
    custom_palette = [plt.cm.tab10(i) for i in range(ncolors_in_palette)]
    custom_palette = custom_palette*int(np.ceil(num_clusters/ncolors_in_palette))
    color_list = [custom_palette[cluster_label] for cluster_label in cluster_labels]
    return color_list, cluster_labels
    
#%% 
# get dataset info
X_featureset_idx, Y_featureset_idx = 1, 0
dataset_name = f'X{X_featureset_idx}Y{Y_featureset_idx}'
dataset_suffix = ''

# load correlation matrix
csv_fname = f'{data_folder}{dataset_name}{dataset_suffix}_correlation_matrix.csv'
corr_mat = pd.read_csv(csv_fname, index_col=0)

# load clustermap 
cluster_df_sorted = pd.read_csv(f'{data_folder}features_by_cluster_corrdist.csv', index_col=0)
num_clusters = 24

# scores to include
scores_to_include = ['fs_avg', 'lasso_distributed', 'nsrc_boost', 'domain_knowledge']
features_to_boost = {
    'Asn':1, 'Gln':0.5, 'Ser':1, 'Thr':1, 'Met':0.5, 'Pro':0.5, 'Gly':0.5, 'Asp':0.5, 'Glu':0.5, 
    'Riboflavin':1, 'Pyridoxine':1, 'Folic acid':0.5, 'Biotin':0.5, 
    'Mn':1, 'Ca':0.5, 'Mg':0.5, 'Zn':0.5, 'Cu':0.5, 
    'Uridine':1, 'Adenosine ':0.5, 'Putrescine':0.5
    }

# scaling factors
fs_avg_weight = 1
lasso_corr_scaling_power = 4
lasso_distributed_scores_weight = 1
nsrc_scaling_factor = 0.2
knowledge_scaling_factor = 0.2 # 0.5

# other parameters
corr_thres = 0.85
num_clusters = 24
num_features_to_select = 10
features_selected_dict = {}

# load feature analysis dataframes
for k, yvar in enumerate(yvar_list_key): 
    print('******************')
    print(yvar)
    print('******************')
    df = pd.read_csv(f'{data_folder}feature_analysis_all_{k}_coefvals.png', index_col=0)
    xvar_list = df.columns.tolist()
    rows = list(df.index)
    scorecard = np.zeros((len(xvar_list),))
    features_selected = []
    features_discarded = []
    
    # get lasso features and selected clusters
    lasso_coefs = df.iloc[4,:]
    lasso_coefs_nonzero = lasso_coefs[lasso_coefs>0]
    lasso_nonzero_xvar = list(lasso_coefs_nonzero.index)
    lasso_nonzero_idxs = np.array([i for i, xvar in enumerate(xvar_list) if xvar in lasso_nonzero_xvar])
    
    # get domain knowledge based features to boost 
    knowledge_features_idxs = []
    for nutrient in features_to_boost.keys():
        matching_idxs = [i for i, xvar in enumerate(xvar_list) if nutrient in xvar]
        knowledge_features_idxs += matching_idxs
        
    # get color list
    color_list, cluster_labels = get_color_list_bycluster(cluster_df_sorted, xvar_list, num_clusters, ncolors_in_palette=10)
    cluster_dict = {xvar:label for xvar, label in zip(xvar_list, cluster_labels)}
    clusters_selected_by_lasso = list(set([cluster_labels[i] for i in lasso_nonzero_idxs]))
    # print(f'{len(clusters_selected_by_lasso)} clusters selected by lasso:', clusters_selected_by_lasso)
    selected_feature_count_bycluster = {label:0 for label in range(1,num_clusters+1)}
    
    # average scores from first 4 rows (MC_rf_SHAP, MC_rf_RFE, MC_rf_coef, MC_plsr_coef)
    if 'fs_avg' in scores_to_include:
        fs_avg = np.nanmean(df.iloc[:4, :].to_numpy(), axis=0)
        scorecard += fs_avg*fs_avg_weight
        plot_colorcoded_barplot(fs_avg, xvar_list, width=0.8, color_list=color_list, annotate_xvar=knowledge_features_idxs, figtitle=f'{yvar}: after fs_avg', savefig=None)
    
    # add lasso distributed scores
    if 'lasso_distributed' in scores_to_include:
        lasso_power = 4 #2
        lasso_distributed_scores = np.zeros((len(lasso_coefs_nonzero), len(xvar_list))) 
        for i, xvar in enumerate(lasso_nonzero_xvar): 
            # get coef value
            coefval = lasso_coefs_nonzero[xvar]
            # get correlation coefficient scores with this xvar
            corr_coefs = corr_mat.loc[xvar,xvar_list]
            # get product of corr_coef and coefval
            product_coefval_corrcoefsq = coefval*(corr_coefs**lasso_corr_scaling_power)
            # add to lasso distributed scores array
            lasso_distributed_scores[i,:] = product_coefval_corrcoefsq
        # get max value for each feature down the column
        lasso_distributed_scores = np.max(lasso_distributed_scores, axis=0)
        scorecard += lasso_distributed_scores*lasso_distributed_scores_weight
        plot_colorcoded_barplot(scorecard, xvar_list, width=0.8, color_list=color_list, annotate_xvar=knowledge_features_idxs, figtitle=f'{yvar}: after lasso_distributed', savefig=None)
        
    # NSRC features
    if 'nsrc_boost' in scores_to_include:
        nsrc_vs_cqa_corr = df.loc['NSRC-vs-CQA_corr',:].to_numpy()
        nsrc_boost_factor = 1 + nsrc_scaling_factor*nsrc_vs_cqa_corr
        # fill nans
        nsrc_boost_factor[np.isnan(nsrc_boost_factor)] = 1
        scorecard *= nsrc_boost_factor
        plot_colorcoded_barplot(scorecard, xvar_list, width=0.8, color_list=color_list, annotate_xvar=knowledge_features_idxs, figtitle=f'{yvar}: after nsrc_boost', savefig=None)
        
    # 'domain knowledge' features
    if 'domain_knowledge' in scores_to_include and yvar.find('Titer')==-1:
        knowledge_boost_factor = np.zeros((len(xvar_list),))
        for nutrient, nutrient_score in features_to_boost.items(): 
            matching_idxs = [i for i, xvar in enumerate(xvar_list) if nutrient in xvar]
            if len(matching_idxs) > 0:
                knowledge_boost_factor[np.array(matching_idxs)] += nutrient_score
        knowledge_boost_factor = 1 + knowledge_boost_factor*knowledge_scaling_factor
        scorecard *= knowledge_boost_factor
        # plot_colorcoded_barplot(scorecard, xvar_list, width=0.8, color_list=color_list, annotate_xvar=knowledge_features_idxs, figtitle=f'{yvar}: after knowledge_boost',  savefig=None)
        
    # commence feature scoring and select top X factors
    plot_colorcoded_barplot(scorecard, xvar_list, width=0.8, color_list=color_list, annotate_xvar=knowledge_features_idxs, figtitle=f'{yvar}: FINAL SCORES',  savefig=None)
    # rank the features by score
    idxs_sorted = np.argsort(scorecard)
    idxs_sorted = idxs_sorted[::-1]
    xvar_list_sorted = [xvar_list[i] for i in idxs_sorted]
    
    
    # reorder correlation matrix by feature rankings, and threshold
    corr_mat_sorted_thres = corr_mat.loc[xvar_list_sorted, xvar_list_sorted]
    corr_mat_sorted_thres = np.tril((corr_mat_sorted_thres>corr_thres)*1)
    np.fill_diagonal(corr_mat_sorted_thres, 0)
    corr_mat_sorted_thres = pd.DataFrame(corr_mat_sorted_thres, columns=xvar_list_sorted, index=xvar_list_sorted)
    fig, ax = plt.subplots(1,1, figsize=(20,20))
    _, _, _ = heatmap(corr_mat_sorted_thres, ax=ax, row_labels=xvar_list_sorted, show_gridlines=False)
    
    # go down the list and select features, adding to the cluster tally
    pointer = 0
    correlated_feature_clusters_selected = []
    while len(features_selected)<num_features_to_select:
        # get next feature to consider
        feature = xvar_list_sorted[pointer]
        select_feature = True
        print(pointer, feature)
        # get correlations with other previously selected features
        corr_mat_sorted_thres_trunc = corr_mat_sorted_thres.loc[features_selected+[feature], features_selected+[feature]]
        sum_highcorr_features_bycol = corr_mat_sorted_thres_trunc.sum(axis=0).to_numpy()
        # find rows where there sum is 2 or more 
        multicorr_cols_idx = np.argwhere(sum_highcorr_features_bycol>=2).reshape(-1,)
        print('multicorr_cols_idx:', multicorr_cols_idx)
        if len(multicorr_cols_idx)>0:
            # iterate through highcorr cols, check if we have a high correlation triad (or more) 
            col_idx_idx = 0
            while select_feature and col_idx_idx<len(multicorr_cols_idx):
                col_idx = multicorr_cols_idx[col_idx_idx]
                col_sum = sum_highcorr_features_bycol[col_idx]
                # reject feature if there are 3 or more highly correlated features with the same col feature 
                if col_sum >= 3: 
                    select_feature = False
                else: 
                    print(f'Checking col {col_idx} (col_sum={col_sum})')
                    # get slice of col corrs and flip it
                    col_corr_flipped = np.flip(corr_mat_sorted_thres_trunc.iloc[:,col_idx].to_numpy())
                    print('col_corr_flipped:', col_corr_flipped)
                    # get indices with 1's
                    row_corr = corr_mat_sorted_thres_trunc.iloc[-1,:len(features_selected)+1].to_numpy()
                    print('row_corr:', row_corr)
                    multicorr_rows_idx = np.argwhere(col_corr_flipped==1).reshape(-1,)
                    print('multicorr_rows_idx:', multicorr_rows_idx)
                    # get sum of highly correlated features in row (for feature under consideration) 
                    row_sum = corr_mat_sorted_thres_trunc.iloc[-1,multicorr_rows_idx].sum()
                    print('Col_sum:', col_sum, 'Row_sum:', row_sum)
                    # sums should be identical to indicate symmetry // mutual correlation of the triad
                    if col_sum==row_sum: 
                        select_feature = False
                col_idx_idx += 1
                
        if select_feature:
            features_selected.append(feature)
            print(f'Selecting feature {pointer}: {feature}')
        else:
            features_discarded.append(feature)
            print(f'Dropping feature {pointer}: {feature}')
        print()
        pointer += 1
    # summary
    features_selected_dict[yvar] = features_selected
    print(f'{len(features_selected)} features selected:', features_selected)
    print()
    print(f'{len(features_discarded)} features discarded:', features_discarded)
    print()
    # get final correlation map
    corr_mat_selected = corr_mat_sorted_thres.loc[features_selected, features_selected]
    fig, ax = plt.subplots(1,1, figsize=(3,3))
    _, _, _ = heatmap(corr_mat_selected, ax=ax, row_labels=features_selected, col_labels=features_selected, show_gridlines=False)


# feature analysis
features_selected_all = []
for yvar, features_selected in features_selected_dict.items():
    print(yvar)
    print(features_selected)
    print()
    features_selected_all += features_selected

xvars_unique, counts = np.unique(np.array(features_selected_all), return_counts=True)
print(len(xvars_unique))
for xvar, count in zip(xvars_unique, counts):
    print(xvar, count)

    
    