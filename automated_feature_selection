#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug  2 20:54:14 2024

@author: charmainechia
"""
import time
import numpy as np
import pandas as pd
from scipy.stats import rankdata
import matplotlib.pyplot as plt
from variables import data_folder, yvar_list_key, features_to_boost_dict, process_features
from model_utils import run_trainval_test
from plot_utils import figure_folder, heatmap
from utils import sort_list, get_XYdata_for_featureset

def parse_model_type(f):
    if f.find('randomforest')>-1:
        model_type = 'randomforest'
    elif f.find('lasso')>-1:
        model_type = 'lasso'
    elif f.find('plsr')>-1:
        model_type = 'plsr'
    return model_type
                
def adapt_dfrow(dfrow_to_adapt, input_suffix, output_suffix, xvar_list_final, xvar_to_ignore=['DO', 'feed %', 'feed vol', 'pH']): 
    dfrow_final = np.zeros((len(xvar_list_final),))
    dfrow_final[:] = np.nan
    for i, xvar_base in enumerate(xvar_list_final): 
        for s in output_suffix:
            xvar_base = xvar_base.replace(s, input_suffix)
        if xvar_base in dfrow_to_adapt and xvar_base not in xvar_to_ignore:
            val = np.abs(dfrow_to_adapt[xvar_base])
            dfrow_final[i] = val
    return dfrow_final

def get_ranking_arr_across_row(val_arr):
    # get rankings by row
    ranking_arr = np.zeros_like(val_arr)
    ranking_arr[:] = np.nan
    for row_idx in range(val_arr.shape[0]):
        ranking_arr[row_idx,:] = val_arr.shape[1]+1-rankdata(val_arr[row_idx,:])
    # set nan elements to NaN
    ranking_arr[np.isnan(val_arr)] = np.nan
    return ranking_arr

def get_contents_of_brackets(string): 
    content_list = []
    string_to_search = string
    no_more_brackets = False
    while len(string_to_search)>3 and not no_more_brackets:
        startidx = string_to_search.find('(')
        if startidx > -1: 
            endidx = string_to_search[startidx:].find(')')
            content = string_to_search[startidx+1:startidx+endidx]
            content_list.append(content)
            string_to_search = string_to_search[startidx+endidx:]
        else: 
            no_more_brackets = True
    return content_list

def plot_colorcoded_barplot(arr, yvar, xvar_list, width=0.8, figsize=(10,5), color_list='b', annotate_xvar=None, figtitle=None, savefig=None):
    # plot barplot
    fig, ax = plt.subplots(1,1, figsize=figsize)
    xtickpos = np.arange(len(xvar_list))
    ax.bar(xtickpos, arr, width=width, color=color_list)
    ax.set_xticks(xtickpos, xvar_list, fontsize=7, rotation=90)
    if figtitle is None:
        ax.set_title(yvar, fontsize=20)
    else: 
        ax.set_title(figtitle, fontsize=20)
    ax.set_ylabel('feature importances', fontsize=16)
    # annotate xvar with lasso features, if needed
    if annotate_xvar is not None: 
        (ymin, ymax) = ax.get_ylim()
        ax.scatter(annotate_xvar, arr[annotate_xvar]+(ymax-ymin)/25, color='k', s=2)
    if savefig is not None:
        plt.savefig(savefig, bbox_inches='tight', dpi=300)
    plt.show()
    
def get_color_list_bycluster(cluster_df, xvar_list, num_clusters=24, ncolors_in_palette=10): 
    cluster_labels = cluster_df.loc[num_clusters,xvar_list].to_numpy()
    custom_palette = [plt.cm.tab10(i) for i in range(ncolors_in_palette)]
    custom_palette = custom_palette*int(np.ceil(num_clusters/ncolors_in_palette))
    color_list = [custom_palette[cluster_label] for cluster_label in cluster_labels]
    return color_list, cluster_labels
    
#%% 
class FeatureSelectionModule: 
    
    def __init__(self, 
                 dataset_name_wsuffix='X1Y0', 
                 yvar_list=yvar_list_key, 
                 knowledge_features_to_boost=features_to_boost_dict, 
                 plot_scorecards=False,
                 num_clusters=24, 
                 data_folder=data_folder,
                 print_progress=True,
                 save_interval=200,
                 csv_fpath=None
                 ):
        self.dataset_name_wsuffix = dataset_name_wsuffix
        self.data_folder = data_folder
        self.yvar_list = yvar_list        
        self.plot_scorecards = plot_scorecards
        self.print_progress = print_progress
        self.fs_param_names = ['fs_avg_weight', 'lasso_corr_scaling_power', 'lasso_distributed_scores_weight', 'nsrc_scaling_factor', 'knowledge_scaling_factor', 'corr_thres', 'num_features_to_select', 'max_num_highcorr_features']
        
        # get data and knowledge features to boost, by yvar
        self.knowledge_features_dict = {}
        self.knowledge_features_idxs_dict = {}
        self.knowledge_features_dict_binary = {}
        self.knowledge_boost_dict = {}
        self.df_dict = {}
        for k, yvar in enumerate(self.yvar_list): 
            # get df
            df = pd.read_csv(f'{data_folder}feature_analysis_all_{k}_coefvals.png', index_col=0)
            self.df_dict[yvar] = df
            # get xvar list (not including process features)
            self.xvar_list = df.columns.tolist()
            
            # get knowledge features to boost
            features_to_boost = knowledge_features_to_boost[yvar]
            knowledge_features = []
            knowledge_features_idxs = []
            knowledge_boost = np.zeros((len(self.xvar_list),))
            for nutrient, nutrient_score in features_to_boost.items():
                matching_features = []
                matching_idxs = []
                for i, xvar in enumerate(self.xvar_list):
                    if nutrient in xvar:
                        matching_features.append(xvar)
                        matching_idxs.append(i)
                if len(matching_idxs) > 0:
                    knowledge_boost[np.array(matching_idxs)] += nutrient_score
                knowledge_features += matching_features
                knowledge_features_idxs += matching_idxs
            self.knowledge_features_dict[yvar] = knowledge_features
            self.knowledge_features_idxs_dict[yvar] = knowledge_features_idxs
            self.knowledge_boost_dict[yvar] = knowledge_boost

        # get color list by cluster
        cluster_df_sorted = pd.read_csv(f'{self.data_folder}features_by_cluster_corrdist.csv', index_col=0)
        self.color_list, cluster_labels = get_color_list_bycluster(cluster_df_sorted, self.xvar_list, num_clusters, ncolors_in_palette=10)

        # get correlation matrix 
        self.corr_mat = pd.read_csv(f'{self.data_folder}{self.dataset_name_wsuffix}_correlation_matrix.csv', index_col=0)

        # feature selection optimization via objective function
        self.ncalls_count = 0
        self.r2_0 = {'Titer (mg/L)_14': 0.924, 'mannosylation_14': 0.798, 'fucosylation_14': 0.753, 'galactosylation_14': 0.945}
        self.objfn_component_names = ['r2_increase', 'frac_knowledge_features', 'frac_allfeatures_selected', 'frac_paired_features', 'corr_avg_selected']
        self.objfn_component_vals = {yvar: {component:0 for component in self.objfn_component_names} for yvar in self.yvar_list}
        self.objfn_component_wts = {
            'r2_increase': -10, 
            'frac_knowledge_features': -2, 
            'frac_allfeatures_selected': 2, 
            'frac_paired_features': -1, 
            'corr_avg_selected': 1
            }
        self.csv = []
        self.save_interval = save_interval
        self.csv_columns = ['i', 'features_selected', 'num_unique_features', 'obj_fn'] + self.objfn_component_names + self.fs_param_names
        self.csv_fpath = csv_fpath
        
        
    def fs_settings_arr2dict(self, fs_settings_arr):
        print(fs_settings_arr)
        fs_settings_dict = {}
        fs_settings_dict['fs_avg_weight'] = fs_settings_arr[0]
        fs_settings_dict['lasso_corr_scaling_power'] = fs_settings_arr[1]
        fs_settings_dict['lasso_distributed_scores_weight'] = fs_settings_arr[2]
        fs_settings_dict['nsrc_scaling_factor'] = fs_settings_arr[3]
        fs_settings_dict['knowledge_scaling_factor'] = fs_settings_arr[4]
        fs_settings_dict['corr_thres'] = fs_settings_arr[5]
        fs_settings_dict['num_features_to_select'] = fs_settings_arr[6] 
        fs_settings_dict['max_num_highcorr_features'] = fs_settings_arr[7]
        return fs_settings_dict

    def get_settings(self, fs_settings, fs):
        
        # get feature selection settings
        if isinstance(fs_settings, np.ndarray):
            fs_settings = self.fs_settings_arr2dict(fs_settings)
        self.fs_settings = fs_settings
        self.fs = fs
        self.fs_avg_weight = fs_settings['fs_avg_weight']
        self.lasso_corr_scaling_power = fs_settings['lasso_corr_scaling_power']
        self.lasso_distributed_scores_weight = fs_settings['lasso_distributed_scores_weight']
        self.nsrc_scaling_factor = fs_settings['nsrc_scaling_factor']
        self.knowledge_scaling_factor = fs_settings['knowledge_scaling_factor']
        self.corr_thres= fs_settings['corr_thres']
        self.num_features_to_select_dict = {yvar: fs_settings['num_features_to_select'] for yvar in self.yvar_list}
        self.MAX_NUM_HIGHCORR_FEATURES_DICT = {yvar: fs_settings['max_num_highcorr_features'] for yvar in self.yvar_list}
            
    def add_feature_importance_scores(self, yvar, df, scorecard):
        # average scores from first 4 rows (MC_rf_SHAP, MC_rf_RFE, MC_rf_coef, MC_plsr_coef)
        fs_avg = np.nanmean(df.iloc[:4, :].to_numpy(), axis=0)
        scorecard += fs_avg*self.fs_avg_weight
        if self.plot_scorecards:
            plot_colorcoded_barplot(scorecard, yvar, self.xvar_list, width=0.8, color_list=self.color_list, annotate_xvar=self.knowledge_features_idxs_dict[yvar], figtitle=f'{yvar}: after fs_avg', savefig=None)
        return scorecard        
        
    def add_distributed_lasso_scores(self, yvar, df, scorecard):
        lasso_coefs = df.iloc[4,:]
        lasso_coefs_nonzero = lasso_coefs[lasso_coefs>0]
        lasso_nonzero_xvar = list(lasso_coefs_nonzero.index)
        lasso_distributed_scores = np.zeros((len(lasso_coefs_nonzero), len(self.xvar_list))) 
        for i, xvar in enumerate(lasso_nonzero_xvar): 
            # get coef value
            coefval = lasso_coefs_nonzero[xvar]
            # get correlation coefficient scores with this xvar
            corr_coefs = self.corr_mat.loc[xvar,self.xvar_list]
            # get product of corr_coef and coefval
            product_coefval_corrcoefsq = coefval*(corr_coefs**self.lasso_corr_scaling_power)
            # add to lasso distributed scores array
            lasso_distributed_scores[i,:] = product_coefval_corrcoefsq
        # get max value for each feature down the column
        lasso_distributed_scores = np.max(lasso_distributed_scores, axis=0)
        scorecard += lasso_distributed_scores*self.lasso_distributed_scores_weight
        if self.plot_scorecards:
            plot_colorcoded_barplot(scorecard, yvar, self.xvar_list, width=0.8, color_list=self.color_list, annotate_xvar=self.knowledge_features_idxs_dict[yvar], figtitle=f'{yvar}: after lasso_distributed', savefig=None)
        return scorecard
    
    def boost_with_domain_knowledge(self, scorecard, yvar):
        knowledge_boost_factor = 1 + self.knowledge_boost_dict[yvar]*self.knowledge_scaling_factor
        scorecard *= knowledge_boost_factor
        return scorecard
    
    def boost_with_NSRC_scores(self, yvar, df, scorecard):
        nsrc_vs_cqa_corr = df.loc['NSRC-vs-CQA_corr',:].to_numpy()
        nsrc_boost_factor = 1 + self.nsrc_scaling_factor*nsrc_vs_cqa_corr
        # fill nans
        nsrc_boost_factor[np.isnan(nsrc_boost_factor)] = 1
        scorecard *= nsrc_boost_factor
        if self.plot_scorecards:
            plot_colorcoded_barplot(scorecard, yvar, self.xvar_list, width=0.8, color_list=self.color_list, annotate_xvar=self.knowledge_features_idxs_dict[yvar], figtitle=f'{yvar}: after nsrc_boost', savefig=None)
        return scorecard
    
    def sort_features_using_scorecard(self, scorecard):
        # rank the features by score
        idxs_sorted = np.argsort(scorecard)
        idxs_sorted = idxs_sorted[::-1]
        self.xvar_list_sorted = [self.xvar_list[i] for i in idxs_sorted]
        # reorder correlation matrix by feature rankings, and threshold
        corr_mat_sorted_thres = self.corr_mat.loc[self.xvar_list_sorted, self.xvar_list_sorted]
        corr_mat_sorted_thres = np.tril((corr_mat_sorted_thres>self.corr_thres)*1)
        np.fill_diagonal(corr_mat_sorted_thres, 0)
        self.corr_mat_sorted_thres = pd.DataFrame(corr_mat_sorted_thres, columns=self.xvar_list_sorted, index=self.xvar_list_sorted)
        # plot heatmap
        if self.plot_scorecards:
            fig, ax = plt.subplots(1,1, figsize=(20,20))
            _, _, _ = heatmap(corr_mat_sorted_thres, ax=ax, row_labels=self.xvar_list_sorted, col_labels=self.xvar_list_sorted, show_gridlines=False, labeltop=True)
    
    def select_features_using_scorecard(self, yvar):
        # go down the list and select features, adding to the cluster tally
        pointer = 0
        features_selected = []
        features_discarded = []
        MAX_NUM_HIGHCORR_FEATURES = self.MAX_NUM_HIGHCORR_FEATURES_DICT[yvar]
        num_features_to_select = self.num_features_to_select_dict[yvar]

        while len(features_selected)<num_features_to_select:
            # get next feature to consider
            feature = self.xvar_list_sorted[pointer]
            select_feature = True
            if self.print_progress: print(pointer, feature)
            # get correlations with other previously selected features
            corr_mat_sorted_thres_trunc = self.corr_mat_sorted_thres.loc[features_selected+[feature], features_selected+[feature]]
            sum_highcorr_features_bycol = corr_mat_sorted_thres_trunc.sum(axis=0).to_numpy()
            # find rows where there sum is 2 or more 
            multicorr_cols_idx = np.argwhere(sum_highcorr_features_bycol>=2).reshape(-1,)
            if self.print_progress: print('multicorr_cols_idx:', multicorr_cols_idx)
            if len(multicorr_cols_idx)>0:
                # iterate through highcorr cols, check if we have a high correlation triad (or more) 
                col_idx_idx = 0
                while select_feature and col_idx_idx<len(multicorr_cols_idx):
                    col_idx = multicorr_cols_idx[col_idx_idx]
                    col_sum = sum_highcorr_features_bycol[col_idx]
                    # reject feature if there are 3 or more highly correlated features with the same col feature 
                    if col_sum >= MAX_NUM_HIGHCORR_FEATURES: 
                        select_feature = False
                    else: 
                        if self.print_progress: print(f'Checking col {col_idx} (col_sum={col_sum})')
                        # get slice of col corrs and flip it
                        col_corr_flipped = np.flip(corr_mat_sorted_thres_trunc.iloc[:,col_idx].to_numpy())
                        if self.print_progress: print('col_corr_flipped:', col_corr_flipped)
                        # get indices with 1's
                        row_corr = corr_mat_sorted_thres_trunc.iloc[-1,:len(features_selected)+1].to_numpy()
                        if self.print_progress: print('row_corr:', row_corr)
                        multicorr_rows_idx = np.argwhere(col_corr_flipped==1).reshape(-1,)
                        if self.print_progress: print('multicorr_rows_idx:', multicorr_rows_idx)
                        # get sum of highly correlated features in row (for feature under consideration) 
                        row_sum = corr_mat_sorted_thres_trunc.iloc[-1,multicorr_rows_idx].sum()
                        if self.print_progress: print('Col_sum:', col_sum, 'Row_sum:', row_sum)
                        # sums should be identical to indicate symmetry // mutual correlation of the triad
                        if col_sum==row_sum: 
                            select_feature = False
                    col_idx_idx += 1
            if select_feature:
                features_selected.append(feature)
                if self.print_progress: print(f'Selecting feature {pointer}: {feature}')
            else:
                features_discarded.append(feature)
                if self.print_progress: print(f'Dropping feature {pointer}: {feature}', end='\n')
            pointer += 1
            
        # summary
        if self.print_progress: 
            print(f'{len(features_selected)} features selected:', features_selected, '\n')
            print(f'{len(features_discarded)} features discarded:', features_discarded, '\n')
        return features_selected
    
    def get_corrmat_for_selected_features(self, features_selected):
        # get final correlation map
        corr_mat_selected = self.corr_mat.loc[features_selected, features_selected]
        corr_mat_selected_thres = self.corr_mat_sorted_thres.loc[features_selected, features_selected]
        if self.plot_scorecards:
            fig, ax = plt.subplots(1,1, figsize=(3,3))
            _, _, _ = heatmap(corr_mat_selected, ax=ax, row_labels=features_selected, col_labels=features_selected, show_gridlines=False, labeltop=True)
        return corr_mat_selected, corr_mat_selected_thres
        
    def analyse_features_selected(self, features_selected_dict):
        features_selected_all = []
        print('features_selected_dict = {')
        for yvar, features_selected in features_selected_dict.items():
            features_selected_all += features_selected
            features_selected += process_features
            print(f"'{yvar}': ", end='')
            print(features_selected, end=',')
            print()
        print('}', '\n')
        # get unique features count
        xvars_unique, counts = np.unique(np.array(features_selected_all), return_counts=True)
        idxs = np.argsort(xvars_unique)
        xvars_unique = xvars_unique[idxs]
        counts = counts[idxs]
        xvars_unique_count_dict = {}
        print(len(xvars_unique))
        for xvar, count in zip(xvars_unique, counts):
            print(xvar, count)
            xvars_unique_count_dict[xvar] = count
        return xvars_unique_count_dict

    def evaluate_model_performance(self, features_selected_dict):
        # get dataset
        X_featureset_idx, Y_featureset_idx = self.dataset_name_wsuffix[1], self.dataset_name_wsuffix[3]
        Y, X, _, _, xvar_list_all = get_XYdata_for_featureset(X_featureset_idx, Y_featureset_idx, dataset_suffix='', data_folder=self.data_folder)
        featureset_suffix = self.fs
        _, kfold_metrics_avg, _ = run_trainval_test(X, Y, yvar_list_key, features_selected_dict, xvar_list_all, self.dataset_name_wsuffix, featureset_suffix, show_plots=self.plot_scorecards, print_progress=False)
        kfold_summary = kfold_metrics_avg.loc[:, ['yvar', 'r2_test_avg']]
        print(kfold_summary)
        kfold_summary = {k: v['r2_test_avg'] for k, v in kfold_summary.set_index('yvar',drop=True).to_dict('index').items()}
        return kfold_summary

    def run_feature_selection(self, 
                            fs_settings, 
                            fs='test', 
                            ): 
        # gets settings
        self.get_settings(fs_settings, fs)
        
        print(f'Implementing setting [{self.fs}]...')
        features_selected_dict = {}
            
        for k, yvar in enumerate(self.yvar_list): 
            print(yvar)
            scorecard = np.zeros((len(self.xvar_list),))
            df = self.df_dict[yvar]
            # add feature importance scores
            scorecard = self.add_feature_importance_scores(yvar, df, scorecard)
            # add lasso distributed scores
            scorecard = self.add_distributed_lasso_scores(yvar, df, scorecard)
            # NSRC features
            scorecard = self.boost_with_NSRC_scores(yvar, df, scorecard)
            # 'domain knowledge' features
            scorecard = self.boost_with_domain_knowledge(scorecard, yvar)
            # plot final scorecard
            if self.plot_scorecards:
                plot_colorcoded_barplot(scorecard, yvar, self.xvar_list, width=0.8, color_list=self.color_list, annotate_xvar=self.knowledge_features_idxs_dict[yvar], figtitle=f'{yvar}: FINAL SCORES',  savefig=None)
            # rank the features by score and reorder correlation matrix
            self.sort_features_using_scorecard(scorecard)

            # go down the sorted scorecard list and select features, checking if there are too many highly-correlated features selected
            features_selected = self.select_features_using_scorecard(yvar)
            corr_mat_selected, corr_mat_selected_thres = self.get_corrmat_for_selected_features(features_selected)
            features_selected_dict[yvar] = features_selected
        
        # feature analysis
        xvars_unique_count_dict = self.analyse_features_selected(features_selected_dict)
            
        # evaluate feature set model performance
        kfold_summary = self.evaluate_model_performance(features_selected_dict)
        
        return features_selected_dict, xvars_unique_count_dict, corr_mat_selected, kfold_summary
    
    def get_frac_paired_features(self, features_selected):
        f_base_count = {}
        for f in features_selected: 
            f_base = f.split('_')[0]
            if f_base not in f_base_count:
                f_base_count[f_base] = 1
            else: 
                f_base_count[f_base] += 1
        num_fbase_paired = 0
        for f, count in f_base_count.items():
            if count > 1: 
                num_fbase_paired += 1
        frac_paired_features = num_fbase_paired*2/len(features_selected)
        return frac_paired_features
    
    def compose_results_df(self, objfun, features_selected_dict, xvars_unique_count_dict):
        
        # initialize row 
        row_dict = {'i': self.ncalls_count}
        
        # format features selected cell
        features_txt = ''
        for i, (yvar, flist) in enumerate(features_selected_dict.items()): 
            features_txt += f'{i}: ' + ', '.join(flist) 
            if i<len(self.yvar_list)-1: 
                features_txt += '\n'
        row_dict.update({'features_selected': features_txt})
        
        # get number of unique features
        row_dict.update({'num_unique_features': len(xvars_unique_count_dict)})
        
        # get overall objective function value 
        row_dict.update({'obj_fn': round(objfun,3)})
        
        # get average objfn component values
        objfn_component_avg = pd.DataFrame(self.objfn_component_vals).transpose().mean(axis=0).round(4).to_dict()
        row_dict.update(objfn_component_avg)
        
        # get feature selection parameters
        row_dict.update(self.fs_settings)
        
        # append to existing data
        self.csv.append(row_dict)
        
        # append to end of existing csv
        if self.csv_fpath is not None and self.ncalls_count%self.save_interval==0:
            # convert list of dicts to dataframe
            self.csv = pd.DataFrame(self.csv, columns=self.csv_columns)
            if self.ncalls_count==self.save_interval: 
                # start a new CSV if this is the first save
                self.csv.to_csv(self.csv_fpath, index=False)
            else:
                # else append to the end of existing file
                self.csv.to_csv(self.csv_fpath, mode='a', index=False, header=False)
            print('Updated CSV.')
            
            # refresh self.csv for next batch of data
            self.csv = []
            
        
    def calculate_objective_function(self,
                                    fs_settings, 
                                    fs=None, 
                                    features_selected_dict=None,
                                    xvars_unique_count_dict=None,
                                    corr_mat_selected=None,
                                    r2=None
                                    ): 
        start = time.time()
        self.ncalls_count += 1
        if fs is None:
            fs = self.ncalls_count
        if features_selected_dict is None and xvars_unique_count_dict is None and corr_mat_selected is None and r2 is None:
            features_selected_dict, xvars_unique_count_dict, corr_mat_selected, r2 = self.run_feature_selection(fs_settings, fs)
        # iterate through yvar list and sum
        objfun = 0
        for yvar in yvar_list_key: 
            # get features selected 
            features_selected = features_selected_dict[yvar]
            # fractional r2_decrease (larger = better)
            self.objfn_component_vals[yvar]['r2_increase'] = (r2[yvar] - self.r2_0[yvar])/self.r2_0[yvar]
            # fraction of 'domain knowledge-suggested' features selected (larger = better)
            features_selected_binary = np.array([1 if xvar in features_selected else 0 for xvar in self.xvar_list])
            self.objfn_component_vals[yvar]['frac_knowledge_features'] = np.sum(features_selected_binary*self.knowledge_boost_dict[yvar])/np.sum(self.knowledge_boost_dict[yvar])
            # compactness (smaller = better)
            self.objfn_component_vals[yvar]['frac_allfeatures_selected'] = len(xvars_unique_count_dict)/len(self.xvar_list)
            # fraction of paired (basal + feed) features both selected (larger = better)
            self.objfn_component_vals[yvar]['frac_paired_features'] = self.get_frac_paired_features(features_selected)
            # total correlations amongst selected features (smaller = better)
            self.objfn_component_vals[yvar]['corr_avg_selected'] = np.mean(corr_mat_selected - np.triu(corr_mat_selected))
            print('\n', f'Objective Function Components ({yvar}):', {k: round(v,3) for k, v in self.objfn_component_vals[yvar].items()})
            
            # sum contributions with weights
            objfun_yvar = 0
            for comp in self.objfn_component_names:
                objfun_yvar += self.objfn_component_vals[yvar][comp] * self.objfn_component_wts[comp]
            objfun += objfun_yvar
        
        print(self.ncalls_count, 'OBJECTIVE FUNCTION (SUM):', round(objfun, 3), '\n')
        end = time.time()
        print(f'Time taken: {round(end-start, 2)} s')
        
        # save results
        self.compose_results_df(objfun, features_selected_dict, xvars_unique_count_dict)
        
        return objfun



#%% 

fs_settings_dict = {
    'naive': {
        'fs_avg_weight': 0,
        'lasso_corr_scaling_power': 1,
        'lasso_distributed_scores_weight': 0,
        'nsrc_scaling_factor': 0,
        'knowledge_scaling_factor': 0,
        'corr_thres': 0.85,
        'num_features_to_select': 5,
        'max_num_highcorr_features': 2
        },
    'model-biased': {
        'fs_avg_weight': 1,
        'lasso_corr_scaling_power': 2,
        'lasso_distributed_scores_weight': 0,
        'nsrc_scaling_factor': 0.2,
        'knowledge_scaling_factor': 0.2,
        'corr_thres': 0.9,
        'num_features_to_select': 10,
        'max_num_highcorr_features': 2
        },
    'knowledge-biased': {
        'fs_avg_weight': 1,
        'lasso_corr_scaling_power': 1,
        'lasso_distributed_scores_weight': 1,
        'nsrc_scaling_factor': 0.5,
        'knowledge_scaling_factor': 2,
        'corr_thres': 0.85,
        'num_features_to_select': 8,
        'max_num_highcorr_features': 2
        },   
    
    }

fs_settings_to_test = ['model-biased', 'knowledge-biased'] #  ['naive'] # 
plot_scorecards = True
print_progress = False
feature_selections_bysetting = {}
for fs in fs_settings_to_test: 
    fs_settings = fs_settings_dict[fs]
    OptFeat = FeatureSelectionModule('X1Y0', yvar_list_key, features_to_boost_dict, plot_scorecards=plot_scorecards, print_progress=print_progress, num_clusters=24)
    # select features
    features_selected_dict, xvars_unique_count_dict, corr_mat_selected, kfold_summary = OptFeat.run_feature_selection(fs_settings, fs)
    # calculate objective function based on selected features
    objfn = OptFeat.calculate_objective_function(fs_settings, fs, features_selected_dict, xvars_unique_count_dict, corr_mat_selected, kfold_summary)
    # update summary dict
    feature_selections_bysetting[fs] = {
        'features_selected': features_selected_dict,
        'model_eval': kfold_summary, 
        'num_unique_features_selected':len(xvars_unique_count_dict),
        'unique_features_selected': xvars_unique_count_dict}
    
print('\n', 'FINAL RESULTS SUMMARY:')
print(feature_selections_bysetting)
    

#%% 
from scipy import optimize

# get bounds of x0 to search
fs_param_names = ['fs_avg_weight', 'lasso_corr_scaling_power', 'lasso_distributed_scores_weight', 'nsrc_scaling_factor', 
                  'knowledge_scaling_factor', 'corr_thres', 'num_features_to_select', 'max_num_highcorr_features']
fs_params_dict_init = {
    'fs_avg_weight': 1,
    'lasso_corr_scaling_power': 2,
    'lasso_distributed_scores_weight': 1,
    'nsrc_scaling_factor': 0.2,
    'knowledge_scaling_factor': 1,
    'corr_thres': 0.85,
    'num_features_to_select': 10,
    'max_num_highcorr_features': 2    
    }
fs_params_bounds_dict = {
    'fs_avg_weight': (0,1),
    'lasso_corr_scaling_power': (1,3),
    'lasso_distributed_scores_weight': (0,1),
    'nsrc_scaling_factor': (0,1),
    'knowledge_scaling_factor': (0,2),
    'corr_thres': (0.8, 1.0),
    'num_features_to_select': (4, 12),
    'max_num_highcorr_features': (1,4)
    }
fs_params_ranges_dict = {
    'fs_avg_weight': slice(0.5,1.5,0.5),
    'lasso_corr_scaling_power': slice(1,4,1),
    'lasso_distributed_scores_weight': slice(0,1.5,0.5),
    'nsrc_scaling_factor': slice(0,1,0.33),
    'knowledge_scaling_factor': slice(0,2.5,0.5),
    'corr_thres': slice(0.8, 0.95,0.05),
    'num_features_to_select': slice(5, 10, 1),
    'max_num_highcorr_features': slice(1,4,1)
    }

fs_params_init = np.array([fs_params_dict_init[param] for param in fs_param_names])
fs_params_bounds = tuple([v for k,v in fs_params_bounds_dict.items()])
fs_params_ranges = tuple([v for k,v in fs_params_ranges_dict.items()])

save_interval = 200
csv_fpath = f'{data_folder}feature_selection_GRIDSEARCH.csv'
OptFeat = FeatureSelectionModule('X1Y0', yvar_list_key, features_to_boost_dict, plot_scorecards=False, num_clusters=24, print_progress=False, save_interval=save_interval, csv_fpath=csv_fpath)
optres = optimize.brute(OptFeat.calculate_objective_function, fs_params_ranges, full_output=True, finish=None)
print('Best fs parameters:', optres[0])
print('Best results:', OptFeat.calculate_objective_function(optres[0]))

